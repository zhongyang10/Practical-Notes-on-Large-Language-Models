Transformer基础架构是Seq2Seq,编码器将输入序列转换成一个隐藏的固定长度的向量表示，解码器将该向量转换成长度不固定的输出序列，Seq2Seq架构的优势有以下几点：
	*端到端学习，直接从原始输入数据学习到有用的表示，并生成期望的输出序列；
	*Seq2Seq模型可以处理可变长度的输入序列，也可以生成可变长度的输出序列；
	*可以很容易和其他神经网络组件结合，以处理更复杂的问题和场景

Seq2Seq架构中，解码器的输出是解码器输入右移一位后的结果，输入以<sos>开始，输出以<eos>结尾，解码器接收两个信息，输出一个信息，输入包括编码器的上下文向量和解码器输入input，输出是解码器输出output，output的每一位由input的预测而来，由此可见，初始时刻，解码器的预测结果非常不稳定，很容易预测错误，第一个词预测错误后，剩下的预测结果几乎都是错的，架构的训练很难收敛

针对以上问题，引入了Teacher Forcing机制，无论输出是否正确，输入都给真值，帮助模型收敛，但是这会引入另外一个问题，模型对输入产生依赖，训练时效果好，测试时效果差，于是有了部分教师强制的策略

Seq2Seq架构首次引入了编码器-解码器架构以及端到端学习，但是该架构存在以下问题：
	*长序列信息损失，当输入序列过长，编码器难以将所有信息压缩成固定的隐藏向量，导致信息损失；
	*训练效率，Seq2Seq架构通常使用RNN作为编码器解码器，训练过程中存在梯度消失和梯度爆炸等问题，影响训练效率和稳定性，此外由于RNN的内部循环结构，只能顺序处理序列，无法对序列整体进行并行计算。
	*原序列和目标序列可能存在对不齐的情况
	
针对以上问题，引入了注意力机制：
	*注意力机制允许模型在解码阶段关注输入序列中的不同部分；
	*注意力机制动态地选择输入序列的关键部分；
	*注意力机制能够为模型提供更精细的词汇级别对其信息；

Transformer逐组件拆解：
	1.多头注意力
	2.逐位置前馈网络
	3.正弦位置编码表
	4.填充位置掩码
	5.编码器层
	6.编码器
	7.后续位置掩码
	8.解码器层
	9.解码器
	10.Transformer
	

